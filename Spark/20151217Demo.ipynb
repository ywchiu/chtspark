{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f24ec0b20d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"file:///tmp/churnTrain.csv\")\n",
    "header = raw_data.first()\n",
    "skip_data = raw_data.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "1313\n"
     ]
    }
   ],
   "source": [
    "splitData = skip_data.randomSplit([0.6,0.4],123)\n",
    "a = splitData[0]\n",
    "print a.count()\n",
    "b = splitData[1]\n",
    "print b.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitlines = skip_data.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'\"1\"',\n",
       "  u'\"KS\"',\n",
       "  u'128',\n",
       "  u'\"area_code_415\"',\n",
       "  u'\"no\"',\n",
       "  u'\"yes\"',\n",
       "  u'25',\n",
       "  u'265.1',\n",
       "  u'110',\n",
       "  u'45.07',\n",
       "  u'197.4',\n",
       "  u'99',\n",
       "  u'16.78',\n",
       "  u'244.7',\n",
       "  u'91',\n",
       "  u'11.01',\n",
       "  u'10',\n",
       "  u'3',\n",
       "  u'2.7',\n",
       "  u'1',\n",
       "  u'\"no\"']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitlines.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,1.0,25.0,265.1,110.0,45.07,197.4,99.0,16.78,244.7,91.0,11.01,10.0,3.0,2.7,1.0]),\n",
       " LabeledPoint(0.0, [0.0,1.0,26.0,161.6,123.0,27.47,195.5,103.0,16.62,254.4,103.0,11.45,13.7,3.0,3.7,1.0])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLine(col):\n",
    "    features = []\n",
    "    churn = col[-1]\n",
    "    international = 0 if col[4] == '\"no\"' else 1\n",
    "    voice = 0 if col[5] == '\"no\"'  else 1\n",
    "    label = 0 if churn == '\"no\"' else 1\n",
    "    features.append(international)\n",
    "    features.append(voice)\n",
    "    features += col[6:-1]\n",
    "    return LabeledPoint(label, Vectors.dense(features))\n",
    "\n",
    "trainData = splitlines.map(parseLine)\n",
    "trainData.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "model = DecisionTree.trainClassifier(trainData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 5 with 57 nodes\n"
     ]
    }
   ],
   "source": [
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, [0.0,1.0,25.0,265.1,110.0,45.07,197.4,99.0,16.78,244.7,91.0,11.01,10.0,3.0,2.7,1.0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = trainData.first()\n",
    "print model.predict(head.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(trainData.map(lambda p: p.features))\n",
    "labels_and_preds = trainData.map(lambda p: p.label).zip(predictions)\n",
    "test_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(trainData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9493\n"
     ]
    }
   ],
   "source": [
    "print \"Test accuracy is {}\".format(round(test_accuracy,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 5 with 57 nodes\n",
      "  If (feature 3 <= 264.7)\n",
      "   If (feature 15 <= 3.0)\n",
      "    If (feature 0 <= 0.0)\n",
      "     If (feature 3 <= 221.4)\n",
      "      If (feature 4 <= 120.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 4 > 120.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 3 > 221.4)\n",
      "      If (feature 6 <= 259.8)\n",
      "       Predict: 0.0\n",
      "      Else (feature 6 > 259.8)\n",
      "       Predict: 1.0\n",
      "    Else (feature 0 > 0.0)\n",
      "     If (feature 13 <= 2.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 13 > 2.0)\n",
      "      If (feature 12 <= 12.9)\n",
      "       Predict: 0.0\n",
      "      Else (feature 12 > 12.9)\n",
      "       Predict: 1.0\n",
      "   Else (feature 15 > 3.0)\n",
      "    If (feature 3 <= 161.3)\n",
      "     If (feature 6 <= 230.2)\n",
      "      If (feature 9 <= 252.7)\n",
      "       Predict: 1.0\n",
      "      Else (feature 9 > 252.7)\n",
      "       Predict: 1.0\n",
      "     Else (feature 6 > 230.2)\n",
      "      If (feature 3 <= 138.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 138.5)\n",
      "       Predict: 0.0\n",
      "    Else (feature 3 > 161.3)\n",
      "     If (feature 6 <= 156.0)\n",
      "      If (feature 3 <= 193.3)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 193.3)\n",
      "       Predict: 0.0\n",
      "     Else (feature 6 > 156.0)\n",
      "      If (feature 3 <= 179.4)\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 > 179.4)\n",
      "       Predict: 0.0\n",
      "  Else (feature 3 > 264.7)\n",
      "   If (feature 1 <= 0.0)\n",
      "    If (feature 6 <= 184.9)\n",
      "     If (feature 9 <= 213.3)\n",
      "      If (feature 3 <= 280.4)\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 > 280.4)\n",
      "       Predict: 0.0\n",
      "     Else (feature 9 > 213.3)\n",
      "      If (feature 6 <= 123.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 6 > 123.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 6 > 184.9)\n",
      "     If (feature 9 <= 123.2)\n",
      "      If (feature 10 <= 103.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 10 > 103.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 9 > 123.2)\n",
      "      If (feature 6 <= 201.3)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 201.3)\n",
      "       Predict: 1.0\n",
      "   Else (feature 1 > 0.0)\n",
      "    If (feature 0 <= 0.0)\n",
      "     If (feature 6 <= 259.8)\n",
      "      Predict: 0.0\n",
      "     Else (feature 6 > 259.8)\n",
      "      If (feature 7 <= 103.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 7 > 103.0)\n",
      "       Predict: 1.0\n",
      "    Else (feature 0 > 0.0)\n",
      "     If (feature 3 <= 280.4)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 280.4)\n",
      "      If (feature 9 <= 229.8)\n",
      "       Predict: 0.0\n",
      "      Else (feature 9 > 229.8)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Learned classification tree model:\"\n",
    "print model.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.820742692936\n",
      "Area under ROC = 0.933551517636\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "metrics = BinaryClassificationMetrics(labels_and_preds)\n",
    "\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1', u'2011-01-01', u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']\n",
      "17379\n"
     ]
    }
   ],
   "source": [
    "path = \"file:///tmp/hour.csv\"\n",
    "raw_data = sc.textFile(path)\n",
    "num_data = raw_data.count()\n",
    "records = raw_data.map(lambda x: x.split(\",\"))\n",
    "first = records.first()\n",
    "print first\n",
    "print num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of first categorical feasture column: {u'1': 0, u'3': 1, u'2': 2, u'4': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "records.cache()\n",
    "\n",
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()\n",
    "\n",
    "\n",
    "print \"Mapping of first categorical feasture column: %s\" % get_mapping(records, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length for categorical features: 57\n",
      "Feature vector length for numerical features: 4\n",
      "Total feature vector length: 61\n"
     ]
    }
   ],
   "source": [
    "# extract all the catgorical mappings\n",
    "mappings = [get_mapping(records, i) for i in range(2,10)]\n",
    "cat_len = sum(map(len, mappings))\n",
    "num_len = len(records.first()[11:15])\n",
    "total_len = num_len + cat_len\n",
    "print \"Feature vector length for categorical features: %d\" % cat_len \n",
    "print \"Feature vector length for numerical features: %d\" % num_len\n",
    "print \"Total feature vector length: %d\" % total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data: [u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']\n",
      "Label: 16.0\n",
      "Linear Model feature vector:\n",
      "[1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.24,0.2879,0.81,0.0]\n",
      "Linear Model feature vector length: 61\n"
     ]
    }
   ],
   "source": [
    "# required imports\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "# function to use the feature mappings to extract binary feature vectors, and concatenate with\n",
    "# the numerical feature vectors\n",
    "def extract_features(record):\n",
    "    cat_vec = np.zeros(cat_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for field in record[2:9]:\n",
    "        m = mappings[i]\n",
    "        idx = m[field]\n",
    "        cat_vec[idx + step] = 1\n",
    "        i = i + 1\n",
    "        step = step + len(m)\n",
    "    \n",
    "    num_vec = np.array([float(field) for field in record[10:14]])\n",
    "    return np.concatenate((cat_vec, num_vec))\n",
    "\n",
    "# function to extract the label from the last column\n",
    "def extract_label(record):\n",
    "    return float(record[-1])\n",
    "\n",
    "data = records.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))\n",
    "first_point = data.first()\n",
    "print \"Raw data: \" + str(first[2:])\n",
    "print \"Label: \" + str(first_point.label)\n",
    "print \"Linear Model feature vector:\\n\" + str(first_point.features)\n",
    "print \"Linear Model feature vector length: \" + str(len(first_point.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree feature vector: [1.0,0.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]\n",
      "Decision Tree feature vector length: 12\n"
     ]
    }
   ],
   "source": [
    "def extract_features_dt(record):\n",
    "    return np.array(map(float, record[2:14]))\n",
    "    \n",
    "data_dt = records.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))\n",
    "first_point_dt = data_dt.first()\n",
    "print \"Decision Tree feature vector: \" + str(first_point_dt.features)\n",
    "print \"Decision Tree feature vector length: \" + str(len(first_point_dt.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method train in module pyspark.mllib.regression:\n",
      "\n",
      "train(cls, data, iterations=100, step=1.0, miniBatchFraction=1.0, initialWeights=None, regParam=0.0, regType=None, intercept=False, validateData=True) method of __builtin__.type instance\n",
      "    Train a linear regression model using Stochastic Gradient\n",
      "    Descent (SGD).\n",
      "    This solves the least squares regression formulation\n",
      "    \n",
      "        f(weights) = 1/(2n) ||A weights - y||^2,\n",
      "    \n",
      "    which is the mean squared error.\n",
      "    Here the data matrix has n rows, and the input RDD holds the\n",
      "    set of rows of A, each with its corresponding right hand side\n",
      "    label y. See also the documentation for the precise formulation.\n",
      "    \n",
      "    :param data:              The training data, an RDD of\n",
      "                              LabeledPoint.\n",
      "    :param iterations:        The number of iterations\n",
      "                              (default: 100).\n",
      "    :param step:              The step parameter used in SGD\n",
      "                              (default: 1.0).\n",
      "    :param miniBatchFraction: Fraction of data to be used for each\n",
      "                              SGD iteration (default: 1.0).\n",
      "    :param initialWeights:    The initial weights (default: None).\n",
      "    :param regParam:          The regularizer parameter\n",
      "                              (default: 0.0).\n",
      "    :param regType:           The type of regularizer used for\n",
      "                              training our model.\n",
      "    \n",
      "                              :Allowed values:\n",
      "                                 - \"l1\" for using L1 regularization (lasso),\n",
      "                                 - \"l2\" for using L2 regularization (ridge),\n",
      "                                 - None for no regularization\n",
      "    \n",
      "                                 (default: None)\n",
      "    \n",
      "    :param intercept:         Boolean parameter which indicates the\n",
      "                              use or not of the augmented representation\n",
      "                              for training data (i.e. whether bias\n",
      "                              features are activated or not,\n",
      "                              default: False).\n",
      "    :param validateData:      Boolean parameter which indicates if\n",
      "                              the algorithm should validate data\n",
      "                              before training. (default: True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "help(LinearRegressionWithSGD.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method trainRegressor in module pyspark.mllib.tree:\n",
      "\n",
      "trainRegressor(cls, data, categoricalFeaturesInfo, impurity='variance', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0) method of __builtin__.type instance\n",
      "    Train a DecisionTreeModel for regression.\n",
      "    \n",
      "    :param data: Training data: RDD of LabeledPoint.\n",
      "                 Labels are real numbers.\n",
      "    :param categoricalFeaturesInfo: Map from categorical feature\n",
      "             index to number of categories.\n",
      "             Any feature not in this map is treated as continuous.\n",
      "    :param impurity: Supported values: \"variance\"\n",
      "    :param maxDepth: Max depth of tree.\n",
      "             E.g., depth 0 means 1 leaf node.\n",
      "             Depth 1 means 1 internal node + 2 leaf nodes.\n",
      "    :param maxBins: Number of bins used for finding splits at each\n",
      "             node.\n",
      "    :param minInstancesPerNode: Min number of instances required at\n",
      "             child nodes to create the parent split\n",
      "    :param minInfoGain: Min info gain required to create a split\n",
      "    :return: DecisionTreeModel\n",
      "    \n",
      "    Example usage:\n",
      "    \n",
      "    >>> from pyspark.mllib.regression import LabeledPoint\n",
      "    >>> from pyspark.mllib.tree import DecisionTree\n",
      "    >>> from pyspark.mllib.linalg import SparseVector\n",
      "    >>>\n",
      "    >>> sparse_data = [\n",
      "    ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      "    ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\n",
      "    ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      "    ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\n",
      "    ... ]\n",
      "    >>>\n",
      "    >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})\n",
      "    >>> model.predict(SparseVector(2, {1: 1.0}))\n",
      "    1.0\n",
      "    >>> model.predict(SparseVector(2, {1: 0.0}))\n",
      "    0.0\n",
      "    >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])\n",
      "    >>> model.predict(rdd).collect()\n",
      "    [1.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTree.trainRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model predictions: [(16.0, 117.89250386724842), (40.0, 116.22496123192109), (32.0, 116.02369145779234), (13.0, 115.67088016754431), (1.0, 115.56315650834314)]\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegressionWithSGD.train(data, iterations=10, step=0.1, intercept=False)\n",
    "true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))\n",
    "print \"Linear Model predictions: \" + str(true_vs_predicted.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree predictions: [(16.0, 54.913223140495866), (40.0, 54.913223140495866), (32.0, 53.171052631578945), (13.0, 14.284023668639053), (1.0, 14.284023668639053)]\n",
      "Decision Tree depth: 5\n",
      "Decision Tree number of nodes: 63\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTree.trainRegressor(data_dt, {})\n",
    "preds = dt_model.predict(data_dt.map(lambda p: p.features))\n",
    "actual = data.map(lambda p: p.label)\n",
    "true_vs_predicted_dt = actual.zip(preds)\n",
    "print \"Decision Tree predictions: \" + str(true_vs_predicted_dt.take(5))\n",
    "print \"Decision Tree depth: \" + str(dt_model.depth())\n",
    "print \"Decision Tree number of nodes: \" + str(dt_model.numNodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error(actual, pred):\n",
    "    return (pred - actual)**2\n",
    "\n",
    "def abs_error(actual, pred):\n",
    "    return np.abs(pred - actual)\n",
    "\n",
    "def squared_log_error(pred, actual):\n",
    "    return (np.log(pred + 1) - np.log(actual + 1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model - Mean Squared Error: 30679.4539\n",
      "Linear Model - Mean Absolute Error: 130.6429\n",
      "Linear Model - Root Mean Squared Log Error: 1.4653\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mse = true_vs_predicted.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae = true_vs_predicted.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle = np.sqrt(true_vs_predicted.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Linear Model - Mean Squared Error: %2.4f\" % mse\n",
    "print \"Linear Model - Mean Absolute Error: %2.4f\" % mae\n",
    "print \"Linear Model - Root Mean Squared Log Error: %2.4f\" % rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Mean Squared Error: 11560.7978\n",
      "Decision Tree - Mean Absolute Error: 71.0969\n",
      "Decision Tree - Root Mean Squared Log Error: 0.6259\n"
     ]
    }
   ],
   "source": [
    "# compute performance metrics for decision tree model\n",
    "mse_dt = true_vs_predicted_dt.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae_dt = true_vs_predicted_dt.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Decision Tree - Mean Squared Error: %2.4f\" % mse_dt\n",
    "print \"Decision Tree - Mean Absolute Error: %2.4f\" % mae_dt\n",
    "print \"Decision Tree - Root Mean Squared Log Error: %2.4f\" % rmsle_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical feature size mapping {0: 5, 1: 3, 2: 13, 3: 25, 4: 3, 5: 8, 6: 3, 7: 5}\n",
      "Decision Tree - Mean Squared Error: 7912.5642\n",
      "Decision Tree - Mean Absolute Error: 59.4409\n",
      "Decision Tree - Root Mean Squared Log Error: 0.6192\n"
     ]
    }
   ],
   "source": [
    "# we create the categorical feature mapping for decision trees\n",
    "cat_features = dict([(i - 2, len(get_mapping(records, i)) + 1) for i in range(2,10)])\n",
    "print \"Categorical feature size mapping %s\" % cat_features\n",
    "# train the model again\n",
    "dt_model_2 = DecisionTree.trainRegressor(data_dt, categoricalFeaturesInfo=cat_features)\n",
    "preds_2 = dt_model_2.predict(data_dt.map(lambda p: p.features))\n",
    "actual_2 = data.map(lambda p: p.label)\n",
    "true_vs_predicted_dt_2 = actual_2.zip(preds_2)\n",
    "# compute performance metrics for decision tree model\n",
    "mse_dt_2 = true_vs_predicted_dt_2.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae_dt_2 = true_vs_predicted_dt_2.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle_dt_2 = np.sqrt(true_vs_predicted_dt_2.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Decision Tree - Mean Squared Error: %2.4f\" % mse_dt_2\n",
    "print \"Decision Tree - Mean Absolute Error: %2.4f\" % mae_dt_2\n",
    "print \"Decision Tree - Root Mean Squared Log Error: %2.4f\" % rmsle_dt_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\",\"Petal.Width\",\"Species\"',\n",
       " u'5.1,3.5,1.4,0.2,\"setosa\"',\n",
       " u'4.9,3,1.4,0.2,\"setosa\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = sc.textFile('file:///tmp/iris.csv')\n",
    "raw_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = raw_data.first()\n",
    "skip_data = raw_data.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 5.1,  3.5,  1.4,  0.2]), array([ 4.9,  3. ,  1.4,  0.2]), array([ 4.7,  3.2,  1.3,  0.2])]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "parsedData = skip_data.map(lambda line: array([float(x) for x in line.split(',')[0:4]]))\n",
    "print parsedData.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "clusters = KMeans.train(parsedData, 3, maxIterations=10,runs=30, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "iris1 = parsedData.first()\n",
    "iris1_predict = clusters.predict(iris1)\n",
    "print iris1_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "prediction = clusters.predict(parsedData)\n",
    "print prediction.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 128.336665429\n"
     ]
    }
   ],
   "source": [
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
